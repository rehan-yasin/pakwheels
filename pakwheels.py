# -*- coding: utf-8 -*-
"""PakWheels.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rNP6IVyEqjFRlkgyTBq9qXG5ko94QQ9w

This program contains code that has been used to develop an ML model that predicts car prices.

Developers: [Ume Rubab, Sardar Rehan Yasin]
"""

#importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import cross_validate, KFold
from sklearn.model_selection import train_test_split

#reading the raw data file
df = pd.read_csv('master.csv')
df.head()

#PREPROCESSING 

#dealing with null Model values
df = df.dropna(subset=['Model'])

#dealing with null Make values
df_null = df[df['Make'].isnull() == True]
for a in df_null.index :
  index = df[df['Model'] == df_null._get_value(a, 'Model')].index
  index = index.to_list()
  if len(index) > 1:
      if index[0] != a:
          df_null._set_value(a, 'Make', df._get_value(index[0], 'Make'))
      else: 
          df_null._set_value(a, 'Make', df._get_value(index[1], 'Make'))
      
for a in df_null.index :
  df._set_value(a,'Make', df_null._get_value(a, 'Make'))
df = df.dropna(subset=['Make'])

#dealing with null Year values
df = df[df['Year'] > 1960]
df = df[df['Year'] < 2025]
df = df.dropna(subset=['Year'])

#dealing with null Engine values
df = df.dropna(subset=['Engine'])

#dealing with null Seating values
df_null = df[df['Seating'].isnull() == True]
for a in df_null.index :
  index = df[df['Model'] == df_null._get_value(a, 'Model')].index
  index = index.to_list()
  if len(index) > 1:
      if index[0] != a:
          df_null._set_value(a, 'Seating', df._get_value(index[0], 'Seating'))
      else: 
          df_null._set_value(a, 'Seating', df._get_value(index[1], 'Seating'))
      
for a in df_null.index :
  df._set_value(a,'Seating', df_null._get_value(a, 'Seating'))
df = df.dropna(subset=['Seating'])

#dealing with null Price values
df['Price']=df.groupby(['Model','Year'])['Price'].apply(lambda x:x.fillna(x.mean()))

if df['Price'].isnull().sum != 0:
  df = df.dropna(subset=['Price'])

df.Make.unique()

#RESOLVING MAKE NAME ISSUES

df.Make = df.Make.str.lower()
def replace_name(a,b):
    df.Make.replace(a,b,inplace=True)
replace_name('land','land rover')

df.Make.unique()

#DROP DUPLICATE VALUES

df.loc[df.duplicated()]
df = df.drop_duplicates(keep='first')
df.count()

df.columns

#MULTIPLE PRICE WITH LAC

df['Price'] = 100000 * df['Price']

#Price Distribution plot 

plt.figure(figsize=(20,8))

plt.subplot(1,2,1)
plt.title('Car Price Distribution Plot')
sns.distplot(df.Price)

plt.subplot(1,2,2)
plt.title('Car Price Spread')
sns.boxplot(y=df.Price)

plt.show()

#Label Encoding of Make and Model

from sklearn.preprocessing import LabelEncoder

Encoder = LabelEncoder()

df.Make= Encoder.fit_transform(df.Make)
df.Make.unique()

df.Model= Encoder.fit_transform(df.Model)
df.Model.unique()

# Finding correlation amongst the variables

df.corr()

# plotting correlation

sns.heatmap(df.corr(), annot=True, fmt='.3f')

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
 
# To scale data
scaler.fit(df)

#PairPlots

sns.pairplot(df)
plt.show()

df.describe()

#dropping Make and Model columns

df=df.drop(columns={"Make"})
df=df.drop(columns={"Model"})

# making heatmap again to observe the correlation better

sns.heatmap(df.corr(), annot=True, fmt='.3f')

#Sorting data chronologically

df.sort_values(['Year'], ascending=(True))

#saving the preprocessed data from raw data into a new .csv file for future use.

df.to_csv('master_pp.csv', index='False')

#updating train and test sets

#indep variables
X = df.iloc[:, 0].values 
#dep variables
y = df.iloc[:,-1].values 
print('Features: ',X)
print('Label: ',y)

fig,ax=plt.subplots(figsize=(6,6))
ax.scatter(X,y)
ax.set_xlabel('Price',fontsize=20)
ax.set_ylabel('Year',fontsize=20)
ax.set_title('Scatter Plot',fontsize=25)

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(df[['Year','Engine','Seating']], df.Price, test_size = 0.2, random_state = 41)

# from sklearn.preprocessing import StandardScaler
# sc = StandardScaler()
# X_train = sc.fit_transform(x_train)
# X_test = sc.transform(x_test)
# print('x-train: ',x_train)
# print('x-test: ',x_test)

#applying a simple LR model to check the output score
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

regression = LinearRegression()
model = regression.fit(x_train, y_train)
y_pred = model.predict(x_test)


# model evaluation
print("mean squared Error: ",mean_squared_error(y_test, y_pred, squared=False))
print("Mean absolute Error: ",mean_absolute_error(y_test, y_pred))
print("R2 score",r2_score(y_test, y_pred))

## Let's find out the model we need to select
maxdegree=10 # The maximum degree we would like to test
training_error=[]
cross_validation_error=[]
for d in range(1,maxdegree):
    x_poly_train=PolynomialFeatures(degree=d).fit_transform(x_train)
    x_poly_test=PolynomialFeatures(degree=d).fit_transform(x_test)
    lr=LinearRegression(fit_intercept=False)
    model=lr.fit(x_poly_train,y_train)
    y_train_pred=model.predict(x_poly_train)
    mse_train=mean_squared_error(y_train,y_train_pred)
    cve=cross_validate(lr,x_poly_train,y_train,scoring='neg_mean_squared_error',cv=7,return_train_score=True)
    training_error.append(mse_train)
    cross_validation_error.append(np.mean(np.absolute(cve['test_score'])))
fig,ax=plt.subplots(figsize=(9,5))
print(cross_validation_error)
ax.scatter(range(1,maxdegree),cross_validation_error, color='Red')
ax.plot(range(1,maxdegree),cross_validation_error)
ax.set_xlabel('Degree',fontsize=20)
ax.set_ylabel('MSE',fontsize=20)
ax.set_title('MSE VS Degree',fontsize=25)

#we observe from the above graph that degree = 7 will be suitable for us.
Poly_Regression = PolynomialFeatures(degree=7 )
poly_var_train = Poly_Regression.fit_transform(x_train)
poly_var_test = Poly_Regression.fit_transform(x_test)
regression = LinearRegression()
model = regression.fit(poly_var_train, y_train)
y_pred = model.predict(poly_var_test)


# model evaluation
print("mean squared Error: ",mean_squared_error(y_test, y_pred, squared=False))
print("Mean absolute Error: ",mean_absolute_error(y_test, y_pred))
print("R2 score",r2_score(y_test, y_pred))

#scatter plot with original and predicted values for Year

plt.scatter(x_test.Year,y_test, color="green")
plt.scatter(x_test.Year,y_pred, color="red") 
plt.xlabel("Year")
plt.ylabel("Price")
plt.title("Year vs Price")

#scatter plot with original and predicted values for Seating 
plt.scatter(x_test.Seating,y_test, color="green")
plt.scatter(x_test.Seating,y_pred, color="red") 
plt.xlabel("Seating")
plt.ylabel("Price")
plt.title("Seating vs Price")

#scatter plot with original and predicted values for Engine

plt.scatter(x_test.Engine,y_test, color="green")
plt.scatter(x_test.Engine,y_pred, color="red") 
plt.xlabel("Engine")
plt.ylabel("Price")
plt.title("Engine vs Price")

#saving the model into a file
import pickle
pkl_filename = "pickle_model.pkl"
with open(pkl_filename, 'wb') as file:
  pickle.dump(model, file)

# to load load model
# with open(pkl_filename, 'rb') as file:
# pickle_model = pickle.load(file)

# applied K-Fold CV method and observed its score which was lower than the used technique hence we have to drop it.
#import sklearn
#from sklearn.model_selection import train_test_split
#from sklearn.preprocessing import PolynomialFeatures
#from sklearn.feature_selection import RFE
#from sklearn.linear_model import LinearRegression
#from sklearn.model_selection import cross_val_score, KFold, GridSearchCV

#from sklearn.model_selection import train_test_split
#x_train, x_test, y_train, y_test = train_test_split(df[['Year','Engine','Seating']], df.Price, test_size = 0.2, random_state = 41)

##################################
####original k-fold
##################################

# step-1: create a cross-validation scheme
#folds = KFold(n_splits = 4, shuffle = True, random_state = 100)

# step-2: specify range of hyperparameters to tune
#hyper_params = [{'n_features_to_select': list(range(1, 14))}]

# step-3: perform grid search
# 3.1 specify model
# Poly_Regression = PolynomialFeatures(degree=7)
# poly_var_train = Poly_Regression.fit_transform(x_train)
# poly_var_test = Poly_Regression.fit_transform(x_test)
# lm = LinearRegression()
# lm.fit(poly_var_train, y_train)
# rfe = RFE(lm)             

# # 3.2 call GridSearchCV()
# model_cv = GridSearchCV(estimator = rfe, 
#                         param_grid = hyper_params, 
#                         scoring= 'r2', 
#                         cv = folds, 
#                         verbose = 1,
#                         return_train_score=True)      

# # fit the model
# model_cv.fit(poly_var_train, y_train)
# y_pred = model_cv.predict(poly_var_test)

# # model evaluation
# print("mean squared Error: ",mean_squared_error(y_test, y_pred, squared=False))
# print("Mean absolute Error: ",mean_absolute_error(y_test, y_pred))
# print("R2 score",r2_score(y_test, y_pred))

